好的，这是根据我们之前所有的讨论、修正和补充，为您整理的一份整合后的、最新的完整版开发设计文档。

---

### **数据库同步与备份自动化平台 - 开发设计文档**

**版本**: 1.0
**日期**: 2025年7月6日

#### **1. 项目概述 (Project Overview)**

**1.1. 目标 (Goal)**
构建一个基于 Web 的自动化平台，用于实现 PostgreSQL 数据库之间的数据同步与备份。平台提供图形化界面，允许用户灵活配置、调度和监控数据同步任务，旨在提高数据管理的效率、可靠性和安全性。

**1.2. 核心功能 (Core Features)**
* **Web管理界面**: 提供直观的界面来管理所有配置和监控任务状态。
* **连接集中管理**: 统一注册和管理所有源、目标数据库的连接信息，并进行加密存储。
* **灵活的任务配置**: 支持全量或增量（通过 WHERE 条件）的数据同步，并可指定具体的 schemas 和 tables。
* **定时任务调度**: 使用 cron 表达式精确控制任务的执行时间。
* **动态表结构创建**: 在数据写入目标库前，能自动检查并根据源表结构创建不存在的表。
* **详尽的日志监控**: 记录每次任务执行的详细过程、状态和错误信息，便于追踪和排错。

**1.3. 目标用户 (Target Users)**
* 数据库管理员 (DBA)
* 开发与运维工程师 (DevOps)

---

#### **2. 系统架构 (System Architecture)**

本平台采用现代化的 Python Web 架构，核心组件包括：

* **Web 前端 (Frontend)**: 使用 **Jinja2** 模板引擎动态生成 HTML 页面，提供用户操作界面。
* **Web 后端 (Backend)**: 基于 **FastAPI** 框架构建，由 **Uvicorn** 以多 worker 模式运行，提供高性能的 API 服务。
* **调度中心 (Scheduler)**: 采用 **APScheduler** 库负责定时任务的调度。
* **同步引擎 (Sync Engine)**: 自定义开发的核心逻辑模块，负责实际的数据库连接、表结构同步和数据流式传输。
* **统一数据库 (Unified Database)**: **一个独立的 PostgreSQL 数据库**，作为整个平台的“大脑”，集中存储以下所有数据：
    * 应用配置（数据库连接、备份任务定义等）
    * APScheduler 的作业存储 (`jobstore`)，利用 PostgreSQL 的行级锁解决多 worker 的并发问题。
    * 所有任务的执行日志。

**交互流程:**
1.  用户通过 Web 界面访问平台。
2.  在“连接管理”页面注册源/目标数据库的连接信息（信息被加密后存入统一数据库）。
3.  在“任务管理”页面创建一个新的同步任务，从下拉列表中选择已配置好的源/目标数据库，并设置同步规则和调度时间。
4.  APScheduler 按时触发任务，并将任务交由一个空闲的 Uvicorn worker 执行。
5.  该 worker 启动“同步引擎”，引擎从统一数据库中读取任务配置，解密并建立与源/目标数据库的连接。
6.  引擎执行表结构比对与创建，然后以流式方式从源库拉取数据，并批量写入目标库。
7.  执行过程中的所有日志（开始、成功、失败、错误详情）被实时写入统一数据库的日志表中。
8.  用户可以在 Web 界面上随时查看任务的执行历史和详细日志。

---

#### **3. 技术栈 (Technology Stack)**

| 分类       | 技术/库                                | 用途                                     |
| :--------- | :------------------------------------- | :--------------------------------------- |
| **后端框架** | FastAPI                                | 构建高性能 API                           |
| **Web 服务器** | Uvicorn                                | ASGI 服务器，用于运行 FastAPI 应用         |
| **数据库 ORM** | SQLAlchemy                             | 与 PostgreSQL 数据库进行交互              |
| **数据库驱动** | psycopg (psycopg2/3)                   | 连接 PostgreSQL                          |
| **数据库迁移** | Alembic                                | 管理数据库表结构的版本变更               |
| **定时任务** | APScheduler                            | 执行定时任务调度                         |
| **模板引擎** | Jinja2                                 | 渲染 Web 页面                            |
| **安全** | Cryptography                           | 加密/解密数据库连接字符串                |
| **数据库** | PostgreSQL                             | 作为整个平台的统一后端存储               |

---

#### **4. 数据模型设计 (Data Model Design)**

所有数据表均创建于“统一数据库”中。

---

#### **5. 详细开发计划 (Detailed Development Plan)**

**阶段 1: 项目基础与模型搭建**
* **目标**: 建立项目骨架，配置好统一的 PostgreSQL 后端，并创建数据模型。
* **任务**:
    1.  初始化 FastAPI 项目结构。
    2.  安装所有技术栈中列出的 Python 依赖，特别是 `cryptography`。
    3.  创建加解密工具函数，并确保密钥从环境变量加载。
    4.  使用 SQLAlchemy 定义上述 4 个数据模型。
    5.  配置 Alembic，并生成、执行第一次数据库迁移，在 PostgreSQL 中创建所有表。

**阶段 2: 数据库连接管理模块 (CRUD)**
* **目标**: 实现对数据库连接信息的增删改查功能。
* **任务**:
    1.  开发用于管理 `DatabaseConnections` 的 FastAPI API 端点 (GET, POST, PUT, DELETE)。
    2.  在 POST/PUT 操作中，对传入的 `connection_string` 进行加密处理。
    3.  创建 Jinja2 模板 (`connections_list.html`, `connection_form.html`) 作为管理界面。

**阶段 3: 核心同步引擎开发**
* **目标**: 开发独立、可测试的、实现“数据库到数据库”同步的核心逻辑。
* **任务**:
    1.  创建 `db_sync_engine.py` 模块。
    2.  实现表结构同步逻辑：使用 SQLAlchemy 的反射功能读取源表结构，并在目标库中动态创建。
    3.  实现数据流式传输：在源库上使用服务器端游标 (`stream_results=True`) 读取数据。
    4.  实现数据批量写入：将流式读取的数据分块，并使用 `bulk_insert_mappings` 或原生 SQL 高效写入目标库。
    5.  封装完整的 `transfer_data(job)` 函数，该函数内部负责解密连接串、建连、同步表、传输数据等全流程。

**阶段 4: 定时任务调度器集成**
* **目标**: 将 APScheduler 集成到应用中，并解决并发执行问题。
* **任务**:
    1.  在 FastAPI 启动事件中初始化 APScheduler。
    2.  配置 `SQLAlchemyJobStore`，使其 `url` 指向本平台的统一 PostgreSQL 数据库。
    3.  编写服务层代码，用于从数据库中加载 `BackupJobs` 并将其动态添加或更新到调度器中。

**阶段 5: 备份任务管理模块 (CRUD)**
* **目标**: 实现对备份任务的增删改查，并在界面上使用已配置好的数据库连接。
* **任务**:
    1.  开发用于管理 `BackupJobs` 和 `JobTargetTables` 的 API 端点。
    2.  更新任务创建/编辑表单 (`job_form.html`)，使用下拉列表展示 `DatabaseConnections` 中的连接供用户选择。
    3.  后端 API 在渲染表单时，需要查询并传递所有可用连接的列表。
    4.  后端 API 在保存任务时，接收的是 `source_db_id` 和 `destination_db_id`。

**阶段 6: 日志与监控功能实现**
* **目标**: 在同步引擎执行过程中嵌入详细的日志记录，并提供查询界面。
* **任务**:
    1.  在 `db_sync_engine.py` 的核心函数中，使用 `try...except...finally` 结构。
    2.  在任务开始时，向 `JobExecutionLogs` 插入状态为 `RUNNING` 的记录。
    3.  在任务成功或失败时，更新该记录的状态、结束时间及详细日志（包括错误堆栈）。
    4.  创建 API 端点和 Web 页面，用于展示某个任务的历史执行日志列表及详情。

**阶段 7: 整合、测试与部署**
* **目标**: 将所有模块联调，进行端到端测试，并准备部署。
* **任务**:
    1.  **全流程测试**: 模拟用户操作，从创建连接、创建任务，到验证自动执行结果和查看日志。
    2.  **异常测试**: 测试数据库连接失败、SQL 语法错误、权限不足等场景，确保系统能正确捕获异常并记录日志。
    3.  **编写 `Dockerfile`**: 将整个应用容器化，便于标准化部署。
    4.  **编写 `README.md`**: 详细说明项目的配置方法、环境变量（特别是数据库连接串和加密密钥）和启动步骤。

---

#### **6. 关键技术与安全考量 (Key Considerations)**

* **安全 (Security)**:
    * **连接加密**: `connection_string` 必须加密存储，防止数据库泄露导致敏感信息暴露。
    * **密钥管理**: 用于加解密的 `ENCRYPTION_KEY` 必须作为机密信息，通过环境变量或专业的密钥管理服务（如 HashiCorp Vault）注入，严禁硬编码在代码中。
* **性能 (Performance)**:
    * **流式读取**: 必须使用服务器端游标来处理大表，避免将整个表加载到应用内存中。
    * **批量写入**: 避免逐条 `INSERT`，应采用 `bulk insert` 或 PostgreSQL 的 `COPY` 命令（如果适用）来提高写入性能。
* **配置管理 (Configuration)**:
    * 遵循 "十二因子应用" (12-Factor App) 原则，所有环境特定的配置（数据库 URI、密钥等）都应通过环境变量提供。
* **健壮性 (Robustness)**:
    * 同步引擎必须有完善的错误处理机制，能够应对网络抖动、数据库临时不可用等问题。
    * 数据库连接应使用连接池进行管理，提高效率和稳定性。